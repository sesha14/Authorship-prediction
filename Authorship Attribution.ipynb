{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from keras.models import Model\n",
    "import chardet\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import one_hot\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from scipy import sparse, stats\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Embedding,Dropout,Conv1D,MaxPooling1D,Flatten,concatenate\n",
    "def split_text(filepath, min_char):\n",
    "    file = open(filepath, \"r\", encoding=\"utf8\")\n",
    "    text = file.read().replace('\\n', ' ')\n",
    "    text = text.replace('--', ' ').replace('. . .', '').replace('_', '')\n",
    "    text = text.replace('.”', '”.').replace('.\"', '\".').replace('?”', '”?').replace('!”', '”!')\n",
    "    file.close()\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    sentences = [sent for sent in sentences if len(sent) >= min_char]\n",
    "    return list(sentences)\n",
    "min_char = 5\n",
    "alcott = split_text('Books/Little_Women.txt', min_char = min_char)\n",
    "stoker = split_text('Books/Dracula.txt', min_char = min_char)\n",
    "twain = split_text('Books/Tom_Sawyer.txt', min_char = min_char)\\\n",
    "        + split_text('Books/Huckleberry_Finn.txt', min_char = min_char)\n",
    "austen = split_text('Books/Pride_and_Prejudice.txt', min_char = min_char)\\\n",
    "         + split_text('Books/Emma.txt', min_char = min_char)\n",
    "collins = split_text('Books/Woman_in_White.txt', min_char = min_char)\n",
    "doyle = split_text('Books/Study_in_Scarlet.txt', min_char = min_char)\\\n",
    "        + split_text('Books/Sign_of_the_Four.txt', min_char = min_char)\\\n",
    "        + split_text('Books/Hound_of_the_Baskervilles.txt', min_char = min_char)\n",
    "montgomery = split_text('Books/Anne_of_Green_Gables.txt', min_char = min_char)\\\n",
    "             + split_text('Books/Anne_of_Avonlea.txt', min_char = min_char)\n",
    "bronte = split_text('Books/Jane_Eyre.txt', min_char = min_char)\n",
    "\n",
    "text_dict = {'Alcott': alcott, 'Stoker': stoker, 'Twain': twain, 'Austen': austen, 'Collins': collins,\n",
    "             'Doyle': doyle, 'Montgomery': montgomery, 'Bronte': bronte }\n",
    "np.random.seed(1)\n",
    "combined = []\n",
    "max_length = 8500\n",
    "names = [alcott, stoker,twain, austen, collins, doyle, montgomery, bronte]\n",
    "for name in names:\n",
    "    name = np.random.choice(name, max_length, replace = False)\n",
    "    combined += list(name)\n",
    "\n",
    "print('Length of the combined list is ', len(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = ['Alcott']*max_length + ['Stoker']*max_length + ['Twain']*max_length + ['Austen']*max_length\\\n",
    "         + ['Collins']*max_length + ['Doyle']*max_length + ['Montgomery']*max_length + ['Bronte']*max_length\n",
    "random.seed(3)\n",
    "zipped = list(zip(combined, labels))\n",
    "random.shuffle(zipped)\n",
    "combined, labels = zip(*zipped)\n",
    "out_data = pd.DataFrame()\n",
    "out_data['text'] = combined\n",
    "out_data['author'] = labels\n",
    "out_data.to_csv('author_data.csv', index=False)\n",
    "with open(\"C:/Users/ashwi/author_attribution-master/author_data.csv\", 'rb') as file:\n",
    "    print(chardet.detect(file.read()))\n",
    "data = pd.read_csv(\"C:/Users/ashwi/author_attribution-master/author_data.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word count and character count lists\n",
    "text = list(data['text'].values)\n",
    "author = list(data['author'].values)\n",
    "word_count = []\n",
    "char_count = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    word_count.append(len(text[i].split()))\n",
    "    char_count.append(len(text[i]))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "word_count = np.array(word_count)\n",
    "char_count = np.array(char_count)\n",
    "\n",
    "# Calculate average word lengths\n",
    "ave_length = np.array(char_count)/np.array(word_count)\n",
    "#def get_stats(var):\n",
    "#    print(\"Min:\", np.min(var))\n",
    "#    print(\"Max:\", np.max(var))\n",
    "#    print(\"Mean:\", np.mean(var))\n",
    "#    print(\"Median\", np.median(var))\n",
    "#    print(\"1st percentile\", np.percentile(var, 1))\n",
    "#    print(\"95th percentile\", np.percentile(var, 95))\n",
    "#    print(\"99th percentile\", np.percentile(var, 99))\n",
    "#    print(\"99.5th Percentile\", np.percentile(var, 99.5))\n",
    "#    print(\"99.9th Percentile\", np.percentile(var, 99.9))\n",
    "#print(\"Word count statistics\")\n",
    "#get_stats(word_count)\n",
    "# Plot word count distribution\n",
    "sns.distplot(word_count, kde = False, bins = 70, color = 'blue').set_title(\"Word Count Distribution\")\n",
    "plt.xlabel('Excerpt Length (Words)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, 200)\n",
    "plt.savefig(\"word_count.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot character count distribution\n",
    "sns.distplot(char_count, kde = False, bins = 100, color = 'blue').set_title(\"Character Count Distribution\")\n",
    "plt.xlabel('Excerpt Length (Characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, 600)\n",
    "plt.savefig(\"char_count.eps\")\n",
    "#print(\"\\nAverage length statistics\")\n",
    "#get_stats(ave_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average excerpt length distribution\n",
    "sns.distplot(ave_length, kde = False, bins = 70, color = 'blue').set_title(\"Average Word Length Distribution\")\n",
    "plt.xlabel('Average Excerpt Length (Characters)')\n",
    "plt.ylabel('Count')\n",
    "plt.xlim(0, 15)\n",
    "plt.savefig(\"ave_length.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "word_outliers = np.where(word_count > 150)\n",
    "word_outliers = np.where(word_count < 2)\n",
    "length_outliers = np.where(ave_length > 10)\n",
    "length_outliers = np.where(ave_length < 3.5)\n",
    "text_string = ''\n",
    "for i in range(len(text)):\n",
    "    text_string += text[i].lower()\n",
    "char_cnt = Counter(text_string)\n",
    "accented_chars = ['ï', 'é', 'ñ', 'è', 'ö', 'æ', 'ô', 'â', 'á', 'à', 'ê', 'ë']\n",
    "accented_text = []\n",
    "for i in range(len(text)):\n",
    "    for j in text[i]:\n",
    "        if j in accented_chars:\n",
    "            accented_text.append(i)    \n",
    "accented_text = list(set(accented_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid character from text\n",
    "text = [excerpt.replace('\\xa0', '') for excerpt in text]\n",
    "unusual_text = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    for j in text[i]:\n",
    "        if j == '\\xa0':\n",
    "            unusual_text.append(i)\n",
    "        \n",
    "unusual_text = list(set(unusual_text))\n",
    " \n",
    "print('There are', str(len(unusual_text)), 'texts containing the invalid character.')\n",
    "new_text = []\n",
    "\n",
    "for excerpt in text:\n",
    "    while \"  \" in excerpt:\n",
    "        excerpt = excerpt.replace(\"  \",\" \")\n",
    "    new_text.append(excerpt)\n",
    "\n",
    "text = new_text\n",
    "ctr = 0\n",
    "for excerpt in text:\n",
    "    if \"  \" in excerpt:\n",
    "        ctr += 1\n",
    "\n",
    "print('There are', ctr, 'excerpts containing blocks of white space.')\n",
    "normed_text = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    new = text[i].lower()\n",
    "    new = new.translate(str.maketrans('','', string.punctuation))\n",
    "    new = new.replace('“', '').replace('”', '')\n",
    "    normed_text.append(new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train, text_test, author_train, author_test = train_test_split(normed_text, author, test_size = 0.2, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "def create_n_grams(excerpt_list, n, vocab_size, seq_size):\n",
    "    n_gram_list = []\n",
    "    for excerpt in excerpt_list:\n",
    "        # Remove spaces\n",
    "        excerpt = excerpt.replace(\" \", \"\")\n",
    "        # Extract n-grams\n",
    "        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]\n",
    "        new_string = \" \".join(n_grams)\n",
    "        # One hot encode\n",
    "        hot = text.one_hot(new_string, round(vocab_size*1.3))\n",
    "        hot_len = len(hot)\n",
    "        if hot_len >= seq_size:\n",
    "            hot = hot[0:seq_size]\n",
    "        else:\n",
    "            diff = seq_size - hot_len\n",
    "            extra = [0]*diff\n",
    "            hot = hot + extra\n",
    "\n",
    "        n_gram_list.append(hot)\n",
    "    \n",
    "    n_gram_array = np.array(n_gram_list)\n",
    "    return n_gram_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(excerpt_list, n, seq_size):\n",
    "    n_gram_list = []\n",
    "    for excerpt in excerpt_list:\n",
    "        excerpt = excerpt.replace(\" \", \"\")\n",
    "        n_grams = [excerpt[i:i + n] for i in range(len(excerpt) - n + 1)]\n",
    "        gram_len = len(n_grams)\n",
    "        if gram_len >= seq_size:\n",
    "            n_grams = n_grams[0:seq_size]\n",
    "        else:\n",
    "            diff = seq_size - gram_len\n",
    "            extra = [0]*diff\n",
    "            n_grams = n_grams + extra\n",
    "        \n",
    "        n_gram_list.append(n_grams)\n",
    "    \n",
    "    # Flatten n-gram list\n",
    "    n_gram_list = list(np.array(n_gram_list).flat)\n",
    "    n_gram_cnt = Counter(n_gram_list)\n",
    "    vocab_size = len(n_gram_cnt)\n",
    "    return vocab_size\n",
    "for i in range(1, 4):\n",
    "    vocab_size = get_vocab_size(text_train, i, 350)\n",
    "    print('Vocab size for n =', i, 'is:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-gram lists\n",
    "gram1_train = create_n_grams(text_train, 1, 52, 350)\n",
    "gram2_train = create_n_grams(text_train, 2, 973, 350)\n",
    "gram3_train = create_n_grams(text_train, 3, 9529, 350)\n",
    "\n",
    "gram1_test = create_n_grams(text_test, 1, 52, 350)\n",
    "gram2_test = create_n_grams(text_test, 2, 973, 350)\n",
    "gram3_test = create_n_grams(text_test, 3, 9529, 350)\n",
    "\n",
    "max_1gram = np.max(gram1_train)\n",
    "max_2gram = np.max(gram2_train)\n",
    "max_3gram = np.max(gram3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def process_data(excerpt_list):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    for excerpt in excerpt_list:\n",
    "        new = excerpt.split()\n",
    "        word_list = [porter.stem(w) for w in new if not w in stop_words]\n",
    "        word_list = \" \".join(word_list)\n",
    "        processed.append(word_list)\n",
    "    \n",
    "    return processed\n",
    "processed_train = process_data(text_train)\n",
    "processed_test = process_data(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words and one hot\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents = 'ascii', stop_words = 'english', min_df = 6)\n",
    "vectorizer.fit(processed_train)\n",
    "\n",
    "# Create feature vectors\n",
    "words_train = vectorizer.transform(processed_train)\n",
    "words_test = vectorizer.transform(processed_test)\n",
    "author_lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "author_lb.fit(author_train)\n",
    "author_train_hot = author_lb.transform(author_train)\n",
    "author_test_hot = author_lb.transform(author_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "def define_model(input_len, output_size, vocab_size, embedding_dim, verbose = True,\n",
    "                drop_out_pct = 0.25, conv_filters = 500, activation_fn = 'relu', pool_size = 2, learning = 0.0001):\n",
    "    inputs1 = Input(shape = (input_len,))\n",
    "    embedding1 = Embedding(vocab_size, embedding_dim)(inputs1)\n",
    "    drop1 = Dropout(drop_out_pct)(embedding1)\n",
    "    conv1 = Conv1D(filters = conv_filters, kernel_size = 3, activation = activation_fn)(drop1)\n",
    "    pool1 = MaxPooling1D(pool_size = pool_size)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "\n",
    "    inputs2 = Input(shape = (input_len,))\n",
    "    embedding2 = Embedding(vocab_size, embedding_dim)(inputs2)\n",
    "    drop2 = Dropout(drop_out_pct)(embedding2)\n",
    "    conv2 = Conv1D(filters = conv_filters, kernel_size = 4, activation = activation_fn)(drop2)\n",
    "    pool2 = MaxPooling1D(pool_size = pool_size)(conv2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "\n",
    "    inputs3 = Input(shape = (input_len,))\n",
    "    embedding3= Embedding(vocab_size, embedding_dim)(inputs3)\n",
    "    drop3 = Dropout(drop_out_pct)(embedding3)\n",
    "    conv3 = Conv1D(filters = conv_filters, kernel_size = 5, activation = activation_fn)(drop3)\n",
    "    pool3 = MaxPooling1D(pool_size = pool_size)(conv3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    merged = concatenate([flat1, flat2, flat3])\n",
    "    output = Dense(output_size, activation = 'softmax')(merged)\n",
    "    model = Model(inputs = [inputs1, inputs2, inputs3], outputs = output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(lr = learning), metrics=['accuracy'])\n",
    "    \n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 1-gram model\n",
    "gram1_model = define_model(350, 8, max_1gram + 1, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 1-gram CNN\n",
    "gram1_model.fit([gram1_train, gram1_train, gram1_train], author_train_hot, epochs=5, batch_size=32, \n",
    "                verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 2-gram model\n",
    "gram2_model = define_model(350, 8, max_2gram + 1, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 2-gram CNN\n",
    "gram2_model.fit([gram2_train, gram2_train, gram2_train], author_train_hot, epochs=5, batch_size=32, \n",
    "                verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 3-gram model\n",
    "gram3_model = define_model(350, 8, max_3gram + 1, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 3-gram CNN\n",
    "gram3_model.fit([gram3_train, gram3_train, gram3_train], author_train_hot, epochs=5, batch_size=32, \n",
    "                verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVC()\n",
    "params = {'kernel': ['linear'], 'C':[1, 10, 100]}\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "grid_obj = GridSearchCV(svm, params, scoring = scorer, verbose = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit bag of words svm\n",
    "np.random.seed(6)\n",
    "word_svm = grid_obj.fit(words_train, author_train)\n",
    "print(word_svm.best_estimator_)\n",
    "print(word_svm.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model2(input_len, output_size, vocab_size, embedding_dim, verbose = True,\n",
    "                drop_out_pct = 0.25, conv_filters = 500, activation_fn = 'relu', pool_size = 2, learning = 0.0001):\n",
    "    # Channel 1\n",
    "    inputs1 = Input(shape = (input_len,))\n",
    "    embedding1 = Embedding(vocab_size, embedding_dim)(inputs1)\n",
    "    drop1 = Dropout(drop_out_pct)(embedding1)\n",
    "    conv1 = Conv1D(filters = conv_filters, kernel_size = 3, activation = activation_fn)(drop1)\n",
    "    pool1 = MaxPooling1D(pool_size = pool_size)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    \n",
    "    # Channel 2\n",
    "    inputs2 = Input(shape = (input_len,))\n",
    "    embedding2 = Embedding(vocab_size, embedding_dim)(inputs2)\n",
    "    drop2 = Dropout(drop_out_pct)(embedding2)\n",
    "    conv2 = Conv1D(filters = conv_filters, kernel_size = 4, activation = activation_fn)(drop2)\n",
    "    pool2 = MaxPooling1D(pool_size = pool_size)(conv2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "\n",
    "    # Channel 3\n",
    "    inputs3 = Input(shape = (input_len,))\n",
    "    embedding3= Embedding(vocab_size, embedding_dim)(inputs3)\n",
    "    drop3 = Dropout(drop_out_pct)(embedding3)\n",
    "    conv3 = Conv1D(filters = conv_filters, kernel_size = 5, activation = activation_fn)(drop3)\n",
    "    pool3 = MaxPooling1D(pool_size = pool_size)(conv3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    \n",
    "    # Channel 4\n",
    "    inputs4 = Input(shape = (input_len,))\n",
    "    embedding4 = Embedding(vocab_size, embedding_dim)(inputs4)\n",
    "    drop4 = Dropout(drop_out_pct)(embedding4)\n",
    "    conv4 = Conv1D(filters = conv_filters, kernel_size = 6, activation = activation_fn)(drop4)\n",
    "    pool4 = MaxPooling1D(pool_size = pool_size)(conv4)\n",
    "    flat4 = Flatten()(pool4)\n",
    "    merged = concatenate([flat1, flat2, flat3, flat4])\n",
    "    output = Dense(output_size, activation = 'softmax')(merged)\n",
    "    model = Model(inputs = [inputs1, inputs2, inputs3, inputs4], outputs = output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adam(lr = learning), metrics=['accuracy'])\n",
    "    if verbose:\n",
    "        print(model.summary())\n",
    "    return model\n",
    "# Create the 3-gram model\n",
    "gram3_model2 = define_model2(350, 8, max_3gram + 1, 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 3-gram CNN\n",
    "gram3_model2.fit([gram3_train, gram3_train, gram3_train, gram3_train], author_train_hot, epochs=5, batch_size=32, \n",
    "                verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)  \n",
    "    df_cm = pd.DataFrame(cm, index = classes,\n",
    "                  columns = classes)\n",
    "    sns.heatmap(df_cm, annot=True, cmap = cmap)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1 (3-gram CNN)\n",
    "\n",
    "t0 = time.time()\n",
    "# Fit model\n",
    "model1 = define_model(350, 8, max_3gram + 1, 600)\n",
    "model1.fit([gram3_train, gram3_train, gram3_train], author_train_hot, epochs=5, batch_size=32, \n",
    "           verbose = 1, validation_split = 0.2)\n",
    "t1 = time.time()\n",
    "\n",
    "# Predict values for test set\n",
    "author_pred1 = model1.predict([gram3_test, gram3_test, gram3_test])\n",
    "t2 = time.time()\n",
    "# Reverse one-hot encoding of labels\n",
    "author_pred1 = author_lb.inverse_transform(author_pred1)\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(author_test, author_pred1)\n",
    "precision, recall, f1, support = score(author_test, author_pred1)\n",
    "ave_precision = np.average(precision, weights = support/np.sum(support))\n",
    "ave_recall = np.average(recall, weights = support/np.sum(support))\n",
    "ave_f1 = np.average(f1, weights = support/np.sum(support))\n",
    "confusion = confusion_matrix(author_test, author_pred1, labels = ['Alcott', 'Stoker', 'Twain', 'Austen', \n",
    "                                                                  'Collins', 'Doyle', 'Montgomery', 'Bronte'])\n",
    "    \n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Ave. Precision:\", ave_precision)\n",
    "print(\"Ave. Recall:\", ave_recall)\n",
    "print(\"Ave. F1 Score:\", ave_f1)\n",
    "print(\"Training Time:\", (t1 - t0), \"seconds\")\n",
    "print(\"Prediction Time:\", (t2 - t1), \"seconds\")\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "plot_confusion_matrix(confusion, classes=['Alcott', 'Stoker', 'Twain', 'Austen', \n",
    "                                                                  'Collins', 'Doyle', 'Montgomery', 'Bronte'],\\\n",
    "                      normalize=True, title='Normalized Confusion Matrix - Model 1')\n",
    "\n",
    "plt.savefig(\"confusion1.eps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2 (Bag of words SVM)\n",
    "np.random.seed(28)\n",
    "t0 = time.time()\n",
    "model2 = SVC(C = 1, kernel = 'linear')\n",
    "model2.fit(words_train, author_train)\n",
    "\n",
    "t1 = time.time()\n",
    "author_pred2 = model2.predict(words_test)\n",
    "\n",
    "t2 = time.time()\n",
    "accuracy = accuracy_score(author_test, author_pred2)\n",
    "precision, recall, f1, support = score(author_test, author_pred2)\n",
    "ave_precision = np.average(precision, weights = support/np.sum(support))\n",
    "ave_recall = np.average(recall, weights = support/np.sum(support))\n",
    "ave_f1 = np.average(f1, weights = support/np.sum(support))\n",
    "confusion = confusion_matrix(author_test, author_pred2, labels =  ['Alcott', 'Stoker', 'Twain', 'Austen', \n",
    "                                                                  'Collins', 'Doyle', 'Montgomery', 'Bronte'])\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Ave. Precision:\", ave_precision)\n",
    "print(\"Ave. Recall:\", ave_recall)\n",
    "print(\"Ave. F1 Score:\", ave_f1)\n",
    "print(\"Training Time:\", (t1 - t0), \"seconds\")\n",
    "print(\"Prediction Time:\", (t2 - t1), \"seconds\")\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "plot_confusion_matrix(confusion, classes=['Alcott', 'Stoker', 'Twain', 'Austen', \n",
    "                                          'Collins', 'Doyle', 'Montgomery', 'Bronte'], \\\n",
    "                      normalize=True, title='Normalized Confusion Matrix - Model 2')\n",
    "\n",
    "plt.savefig(\"confusion2.eps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
